# Bundestag Tiktoken training and sampling configuration.
# Notes on interplay:
# - Effective tokens per optimizer update = batch_size * grad_accum_steps * block_size.
# - Memory roughly scales with n_layer and quadratically with block_size; also linearly with batch_size.
# - [train.data].block_size must be <= [train.model].block_size (model context capacity).
# - If [train.schedule].decay_lr = false, warmup/decay/min_lr are ignored.
# - Paths (dataset_dir, out_dir) are used exactly as configured (absolute or relative), no rewriting.

[train.model]
# Number of Transformer blocks; increases capacity and compute/memory cost.
n_layer = 6
# Number of attention heads per layer; must divide n_embd; more heads can help stability at added cost.
n_head = 8
# Embedding/hidden size; larger increases model capacity and compute.
n_embd = 384
# Model context length (positional embeddings); must be >= [train.data].block_size.
block_size = 512
# Dropout rate for regularization during training; 0.0 for deterministic/fast runs.
dropout = 0.0
# Whether to include bias terms in linear/LayerNorm layers; false is common for GPT-style models.
bias = false

[train.data]
# Folder containing the preprocessed dataset files referenced below. Used as-is.
dataset_dir = "ml_playground/experiments/bundestag_tiktoken/datasets"
# Training data file name (relative to dataset_dir).
train_bin = "train.bin"
# Validation data file name (relative to dataset_dir).
val_bin = "val.bin"
# Metadata (e.g., vocab/encoding) file name (relative to dataset_dir).
meta_pkl = "meta.pkl"
# Micro-batch size per optimization step; memory ~ batch_size * block_size^2.
batch_size = 64
# Sequence length of each training sample; must be <= [train.model].block_size.
block_size = 128
# Number of gradient accumulation steps; effective batch (samples) = batch_size * grad_accum_steps.
# Effective tokens per optimizer update = (batch_size * grad_accum_steps) * block_size.
grad_accum_steps = 2

[train.optim]
# Base learning rate used by the optimizer; interacts with schedule below if decay_lr = true.
learning_rate = 0.0006
# L2-style weight decay for regularization; often excluded for LayerNorm/bias in AdamW.
weight_decay = 0.1
# Adam/AdamW beta1 (momentum) parameter; higher retains more past gradients.
beta1 = 0.9
# Adam/AdamW beta2 (variance) parameter; higher smooths more but reacts slower to changes.
beta2 = 0.95
# Global gradient norm clipping threshold to prevent exploding gradients (0 disables if supported).
grad_clip = 1.0

[train.schedule]
# Enable learning-rate schedule (warmup + decay). If false, other fields are ignored.
decay_lr = true
# Number of warmup iterations from 0 -> learning_rate.
warmup_iters = 1000
# Iterations over which LR decays from learning_rate down toward min_lr.
# Interplay with [train.runtime].max_iters: if much smaller, LR stays at min for long.
lr_decay_iters = 1000000
# Lower bound (floor) for the learning rate during decay.
min_lr = 0.00006

[train.runtime]
# Output directory for logs, checkpoints, and artifacts. Used as-is.
out_dir = "ml_playground/experiments/bundestag_tiktoken/out/bundestag_tiktoken"
# Total number of optimizer iterations to run (across all accumulation steps).
max_iters = 1000000
# How often (in iterations) to run evaluation/checkpoint logic.
eval_interval = 300
# Number of evaluation batches to average per evaluation; higher reduces noise but takes longer.
eval_iters = 100
# How often (in iterations) to log training metrics.
log_interval = 100
# If true, only runs evaluation and exits (no training updates).
eval_only = false
# If true, save a checkpoint on every eval (last and/or best/top-k as configured).
always_save_checkpoint = false
# Random seed for reproducibility (data shuffling, init, sampling).
seed = 1337
# Compute device backend: "cuda" (NVIDIA), "mps" (Apple Silicon), or "cpu".
device = "mps"
# Numeric precision for tensors and parameters; affects speed/memory vs. stability/accuracy.
dtype = "float32"
# If true, compile/optimize the model/graph (when supported); speeds up but increases startup time.
compile = false

# Checkpoint policy
# Filename for the most recent checkpoint snapshot saved (rolling "last" save).
ckpt_last_filename = "ckpt_last.pt"
# Filename for the best-scoring checkpoint according to ckpt_metric.
ckpt_best_filename = "ckpt_best.pt"
# Keep the top-K best checkpoints (by ckpt_metric) in addition to "last"; 0 disables.
ckpt_top_k = 0
# Metric key to select "best" (e.g., "val_loss"); must match what evaluation reports.
ckpt_metric = "val_loss"
# Whether higher metric values are better (true) or lower are better (false); false for "val_loss".
ckpt_greater_is_better = false
# Atomic write of checkpoint files to avoid partial files on crashes.
ckpt_atomic = false
# If true, write extra metadata (e.g., config, metrics) alongside checkpoints for reproducibility.
ckpt_write_metadata = true
# Minimum time between time-based checkpoint saves; 0 disables time-based saves.
ckpt_time_interval_minutes = 10
# Exponential smoothing coefficient for the metric used to decide "best"; 0.0 disables smoothing.
best_smoothing_alpha = 0.0
# Early stopping patience in number of evaluations without improvement; 0 disables early stopping.
early_stop_patience = 0
# Exponential Moving Average (EMA) decay for model weights; 0.0 disables.
ema_decay = 0.0

[sample.runtime]
# Output directory to read checkpoints from (and optionally write samples/logs to).
# Typically should match the training run's out_dir you want to sample from.
out_dir = "ml_playground/experiments/bundestag_tiktoken/out/bundestag_tiktoken"
# Iteration controls are usually inert for pure sampling, but kept for API symmetry.
max_iters = 0
# Evaluation cadence placeholders (often unused in pure sampling flows).
eval_interval = 1
# Number of evaluation batches during sampling flows (often unused).
eval_iters = 1
# Logging cadence during sampling (if the driver logs progress).
log_interval = 1
# If true, run evaluation-only behavior in sampling loop implementations that support it.
eval_only = false
# Whether to save artifacts during sampling (e.g., generated text dumps); false to avoid clutter.
always_save_checkpoint = false
# Seed to make generation reproducible for a fixed prompt and settings.
seed = 1337
# Device to run generation on; "mps" uses Apple Silicon; use "cuda"/"cpu" as available.
device = "mps"
# Numeric precision during sampling; can be lower precision if supported to speed up generation.
dtype = "float32"
# Whether to compile/optimize the model for inference (if supported); usually false for quick runs.
compile = false

[sample.sample]
# Prompt to prime the model; generation continues after this text.
start = "\nNächste Rednerin ist die Vorsitzende der AfD-Fraktion, Dr. Alice Weidel.\n(Beifall bei der AfD)\n  Dr. Alice Weidel (AfD): \n"
# Number of independent samples to generate for the same prompt/settings.
num_samples = 1
# Maximum number of new tokens to generate after the prompt.
max_new_tokens = 1024
# Softens or sharpens the probability distribution before sampling; higher = more random, 0 = greedy/argmax.
temperature = 0.8
# Sample only from the top-K most likely tokens at each step; lower K increases coherence, higher K increases diversity.
top_k = 200
