# Bundestag Tiktoken training and sampling configuration.
# Notes on interplay:
# - Effective tokens per optimizer update = batch_size * grad_accum_steps * block_size.
# - Memory roughly scales with n_layer and quadratically with block_size; also linearly with batch_size.
# - [train.data].block_size must be <= [train.model].block_size (model context capacity).
# - If [train.schedule].decay_lr = false, warmup/decay/min_lr are ignored.
# - Paths (dataset_dir, out_dir) are used exactly as configured (absolute or relative), no rewriting.

[train.model]
# token-level / tiktoken workflow - small model for fast experiments
# n_layer: number of Transformer blocks. More layers => more capacity and memory/compute.
n_layer = 4
# n_head: attention heads per layer. Must divide n_embd. More heads can improve modelling but increase memory.
n_head = 4
# n_embd: hidden/embedding size. Increases representation capacity; scales memory linearly.
n_embd = 128
# block_size: model's maximum context length (in tokens). Attention memory/time scales ~O(block_size^2).
# Keep this >= [train.data].block_size.
block_size = 64
# dropout: dropout probability applied during training for regularization (0.0 = disabled).
dropout = 0.1
# bias: whether linear/LayerNorm layers include bias terms. Turning off slightly reduces params.
bias = false
# vocab_size: reserve to 4096 based on ~3825 discovered tokens (+headroom for specials)
vocab_size = 4096

[train.data]
# Character-level n-gram size used during dataset preparation (1 = single/chars, >1 = n-grams).
# For tiktoken/BPE workflows set this to 1 (preparer treats n <= 1 as token mode).
ngram_size = 1
# Micro-batch size per optimization step (per device)
batch_size = 8
# Sequence length; ensure <= [train.model].block_size
block_size = 64
# Number of gradient accumulation steps to increase effective batch without extra memory
grad_accum_steps = 4
# Sampling policy: random sampling for tokenized workflows
sampler = "random"


[train.optim]
# learning_rate: base LR for optimizer. Larger models often require smaller LR; tune for stability.
learning_rate = 0.0005
# weight_decay: L2-style regularization applied to weights (not typically to LayerNorm/bias).
weight_decay = 0.1
# beta1 / beta2: Adam optimizer moment coefficients.
# - beta1 (default ~0.9) controls exponential decay for the 1st moment (momentum). Lowering slightly (e.g., 0.8)
#   can help with very noisy gradients; raising (closer to 1.0) increases smoothing of past gradients.
# - beta2 (default ~0.95-0.999) controls decay for the 2nd moment (variance). Lower values react faster to changes.
# Typical stable defaults: beta1=0.9, beta2=0.95 for transformer-style training; reduce beta2 if you observe slow adapt.
beta1 = 0.9
beta2 = 0.95
# grad_clip: clip gradient norm to this value (prevents exploding gradients). 0 or None disables if supported.
grad_clip = 1.0

[train.schedule]
# decay_lr: enable/disable LR schedule. True uses warmup + decay (recommended).
decay_lr = true
# warmup_iters: number of iterations to linearly increase LR from 0 to learning_rate.
# Recommended: a few thousand iterations (e.g., 1k-5k) depending on dataset size and batch config.
# Warmup stabilizes early training by avoiding large updates from initial noisy gradients.
warmup_iters = 2000
# lr_decay_iters: total iterations over which LR decays toward min_lr after warmup.
# For cosine or linear decay, set this near your expected max_iters to get a smooth decay over training.
# If set much smaller than max_iters, LR will hit min_lr early and stay there; if much larger, decay will be slow.
lr_decay_iters = 200_000
# min_lr: lower bound for LR during decay. Prevents LR from going to zero and allows continued small updates.
# Typical values: 1e-5 .. 1e-6 for mid-sized training runs; lower for very long schedules.
min_lr = 1e-5

[train.runtime]
# max_iters: total optimizer steps (after accumulation) to run.
max_iters = 200_000
# eval_interval: run validation/checkpoint logic every this many iterations.
# Less frequent evals reduce overhead and avoid triggering early stop too soon.
eval_interval = 1000
# eval_iters: number of validation batches to average when computing val metrics (higher -> smoother estimates).
# Keep a reasonable average without spending too long in eval.
eval_iters = 100
# log_interval: how often (iterations) to print training metrics.
log_interval = 1
# eval_only: if true, run evaluation and exit without training updates.
eval_only = false
# always_save_checkpoint: if true, save checkpoint on every eval (increases disk usage).
always_save_checkpoint = false
# seed: RNG seed for reproducibility of splits, initialization, and sampling.
seed = 1337
# device: compute backend. Use "mps" for Apple Silicon, "cuda" for NVIDIA, or "cpu".
device = "mps"
# dtype: numeric precision for tensors. float16/bfloat16 reduce memory but may affect numeric stability.
dtype = "bfloat16"
# compile: whether to JIT-compile/optimize model graphs (if supported). Speeds up iterations at cost of startup time.
compile = false
# ckpt_top_k: keep top-K best checkpoints by ckpt_metric (0 disables retention).
ckpt_top_k = 1
# ckpt_metric: metric key used to select "best" checkpoint (e.g. "val_loss").
ckpt_metric = "val_loss"
# ckpt_greater_is_better: True if larger metric values are better (e.g., accuracy); False for metrics like loss.
ckpt_greater_is_better = false
# ema_decay: EMA decay applied to model weights for more stable evaluation. 0 disables EMA.
ema_decay = 0.999
# best_smoothing_alpha: smoothing factor for metric used to decide 'best' (0 disables smoothing).
best_smoothing_alpha = 0.9
# early_stop_patience: number of evals without improvement before early stopping; 0 disables.
early_stop_patience = 0

[sample.runtime]
# out_dir: checkpoint directory to load from for sampling flows (keep aligned with train out_dir if sampling trained models).
out_dir = "./out"
# max_iters, eval_interval, eval_iters: retained for symmetry with training interface; often unused during pure sampling.
max_iters = 0
eval_interval = 1
eval_iters = 1
# log_interval: logging cadence during sampling (if driver logs).
log_interval = 1
# eval_only: if true, run sampling-only evaluation behaviors supported by the driver.
eval_only = false
# always_save_checkpoint: whether sampling should also persist artifacts (usually false).
always_save_checkpoint = false
# seed: RNG seed for reproducible generation for the same prompt/settings.
seed = 1337
# device: compute backend for generation. "mps" recommended on Apple Silicon for speed.
device = "mps"
# dtype: numeric precision during sampling. float16 speeds inference but watch stability; float32 is safest.
dtype = "bfloat16"
# compile: compile/optimize the model for inference (if supported).
compile = false

[sample.sample]
# Prompt to prime the model; generation continues after this text.
start = "\nNächste Rednerin ist die Vorsitzende der AfD-Fraktion, Dr. Alice Weidel.\n(Beifall bei der AfD)\n  Dr. Alice Weidel (AfD): \n"
# Number of independent samples to generate for the same prompt/settings.
num_samples = 2
# Maximum number of new tokens to generate after the prompt.
max_new_tokens = 512
# Softens or sharpens the probability distribution before sampling; higher = more random, 0 = greedy/argmax.
temperature = 0.8
# Sample only from the top-K most likely tokens at each step; lower K increases coherence, higher K increases diversity.
top_k = 200
