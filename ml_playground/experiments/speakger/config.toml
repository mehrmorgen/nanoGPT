# SpeakGer Fine-tuning (Gemma 3 270M + LoRA on MPS)
# This config drives a 3-stage pipeline: prepare -> train -> sample.
# Optimized for local training on Apple Silicon (32GB RAM).
# For different model sizes, change model_name to:
#   - google/gemma-2-2b (default, good balance)
#   - google/gemma-2-9b-it (larger, may require lower batch sizes)
#   - google/gemma-2-2b-it (instruction-tuned variant)
# Notes on interplay:
# - Effective tokens per optimizer update = batch_size * grad_accum_steps * block_size.
# - Memory roughly scales with model size and quadratically with block_size; also linearly with batch_size.
# - [train.data].block_size should be <= [train.hf_model].block_size.
# - If [train.schedule].decay_lr = false, warmup/decay/min_lr are ignored.
# - Paths (raw_dir, dataset_dir, out_dir) are used exactly as configured (absolute or relative), no rewriting.

[prepare]
# Name of the dataset pipeline to use.
dataset = "gemma_finetuning_mps"
# Path to a folder of raw .txt files (recursively scanned) OR a single CSV file.
# Example CSV: raw_dir = "ml_playground/datasets/Bundestag.csv"
# UPDATE THIS PATH to point to your dataset location.
raw_dir = "ml_playground/experiments/speakger/raw"
# Directory where the prepared dataset artifacts are written (tokenizer, train.jsonl, val.jsonl, meta.json).
dataset_dir = "ml_playground/experiments/speakger/datasets"
# Whether to wrap speeches with structure tokens (SPEECH_START/END) during preparation.
add_structure_tokens = true
# Token string used to separate concatenated documents during packing.
doc_separator = "<DOC_SEP>"

[train.hf_model]
# Hugging Face Gemma model to fine-tune with LoRA.
# Options: google/gemma-2-2b (2B params, recommended), google/gemma-2-9b-it (9B params), etc.
model_name = "google/gemma-3-270m-it"
# Enable gradient checkpointing to reduce memory at the cost of additional compute.
gradient_checkpointing = true
# Logical sequence length for data packing/truncation during training (tokens per training block).
# Conservative setting for 32GB RAM.
block_size = 128

[train.peft]
# Toggle LoRA training. If false, trains full model (not recommended on consumer hardware).
enabled = true
# Intrinsic rank of LoRA matrices (capacity of the adapter).
r = 8
# Scaling factor for LoRA updates (effective update magnitude).
lora_alpha = 16
# Dropout applied to LoRA layers (regularization).
lora_dropout = 0.05
# Whether to learn bias terms: "none", "all", or "lora_only".
bias = "none"
# Target module names within the transformer blocks to apply LoRA to.
# Gemma uses q_proj, k_proj, v_proj, o_proj for attention layers.
target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]
# If true, also apply LoRA to MLP projections (up_proj/down_proj/gate_proj).
extend_mlp_targets = false

[train.model]

[train.data]
# Directory containing prepared dataset artifacts (tokenizer/, train.jsonl, val.jsonl, meta.json).
dataset_dir = "ml_playground/experiments/speakger/datasets"
# Per-step batch size (before gradient accumulation). Conservative for 32GB RAM + MPS.
batch_size = 6
# Number of micro-steps to accumulate gradients before an optimizer step.
grad_accum_steps = 10
# Token block length for training examples (should usually match hf_model.block_size).
block_size = 128
# Whether to shuffle training blocks each epoch (set false for reproducible debug runs).

[train.optim]
# Base learning rate for AdamW optimizer. Slightly lower for Gemma than Qwen.
learning_rate = 0.15
# L2 weight decay strength (0 disables).
weight_decay = 0.0
# AdamW beta1 (momentum) coefficient.
beta1 = 0.9
# AdamW beta2 (variance) coefficient.
beta2 = 0.95
# Clip global gradient norm to this value (0 or null to disable).
grad_clip = 1.0

[train.schedule]
# Enable cosine decay schedule (with warmup).
decay_lr = true
# Number of warmup iterations at the start of training.
warmup_iters = 100
# Iteration index at which LR decays to min_lr (spans warmup_iters..lr_decay_iters).
# Conservative for local training.
lr_decay_iters = 10000
# Minimum learning rate at the end of decay.
min_lr = 0.015

[train.runtime]
# Output directory for checkpoints, adapters, tokenizer copy, logs, and samples.
out_dir = "out/speakger_gemma3_270m_lora_mps"
# Total number of optimizer steps to run. Conservative for local training.
max_iters = 160000
# Frequency (in steps) to run evaluation on the validation set.
eval_interval = 100
# Number of validation batches to evaluate per evaluation step.
eval_iters = 50
# Frequency (in steps) to print training logs.
log_interval = 10
# If true, run a single evaluation and exit (no training).
eval_only = false
# If true, always write checkpoints at evaluation steps (best/last pointers are maintained).
always_save_checkpoint = true
# Global random seed for Python and torch.
seed = 1337
# Compute device: "mps" (Apple Silicon), "cuda" (NVIDIA), or "cpu".
device = "mps"
# Compute dtype: "float16" recommended on mps/cuda for memory and speed; "float32" for numerical stability.
dtype = "float16"
# If true, compile the model (PyTorch 2.x) to potentially speed up training.
# May cause issues on MPS, recommended to keep false for now.
compile = false
# Also checkpoint periodically based on wall-clock minutes (0 to disable).
ckpt_time_interval_minutes = 15
# If true, write checkpoints atomically (temp dir then rename) to avoid partial writes.
ckpt_atomic = true
# Exponential smoothing factor for tracking "best" metric (0 disables smoothing).
best_smoothing_alpha = 0.0
# Early stopping patience in evaluations without improvement (<=0 disables early stopping).
early_stop_patience = 3
# Exponential moving average decay for model parameters (0 disables EMA).
ema_decay = 0.0
# If true, save a fully merged model when achieving best validation (in addition to adapters).
save_merged_on_best = false
# Keep only the latest N iter_* checkpoints (0 disables pruning).
keep_last_n = 2

[sample.runtime]
# Directory from which to read adapters/tokenizer and where to write generated samples.
out_dir = "out/speakger_gemma3_270m_lora_mps"
# Compute device for sampling: "mps", "cuda", or "cpu".
device = "mps"
# Compute dtype during sampling; "float16" is common for speed/memory.
dtype = "float16"
# If true, torch.compile the model before generation (may add startup overhead).
compile = false
# Random seed applied before sampling to make runs reproducible (when do_sample=true).
seed = 1337
# Require LoRA adapters during sampling (error if missing)
require_adapters = true

[sample.sample]
# Prompt text to start generation; can also be "FILE:/path/to/prompt.txt".
# Example prompt showcasing SpeakGer metadata conditioning for historic debate style.
start = "\nSprecher: Jan Korte (DIE LINKE)\nThema: Kampf gegen den PlÃ¼schtier-Terrorismus\nJahr: 2018\n\nMeine Damen und Herren,"
# Number of samples to generate per run.
num_samples = 1
# Maximum number of new tokens to generate (beyond the prompt).
max_new_tokens = 1024
# Softness of sampling; higher -> more random (set 0 for greedy).
temperature = 0.8
# Top-k sampling: restrict next-token choices to the k most likely (0 disables).
top_k = 200
# Nucleus sampling: restrict to smallest set of tokens whose cumulative prob >= top_p (1.0 disables).
top_p = 0.9